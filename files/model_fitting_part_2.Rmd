---
title: "Model Fitting Part 2"
author: "Joe Martin"
date: "10/18/2021"
output: pdf_document
---
## Introduction

The goal of this section is to determine whether it is possible to predict nausea based on other symptoms a patient is displaying.  This is first tested with all present variables as predictors, then tested against the main predictor of interest, Runny Nose.

```{r load, echo = FALSE}
pacman::p_load(pacman, tidyverse, tidymodels, here, readr, broom.mixed, skimr, rsample)

data <- here::here("data","processed_data","processeddata.rds")
df <- read_rds(data)
```

```{r}
# Write code that takes the data and splits it randomly into a train and test that, following for instance the example in the Data Splitting section of the Get Started tidymodels tutorial.

# I messed this part up and created a new Body Temp variable. This is unnecessary, but I'm leaving it.

set.seed(2)

df$high_temp <- ifelse(df$BodyTemp > 99, "high_temp","normal_range")
df$high_temp <- factor(df$high_temp)

# Use 70/30 split on data

data_split <- initial_split(df, prop = 7/10)

train_data <- training(data_split)
test_data <- testing(data_split)
```

## Testing All Predictors

The following code generates recipes for the training and test sets of data, then builds a workflow to fit to a logistic model to all predictor variables. 
```{r}
# Next, following the example in the Create Recipes section of the Get Started tidymodels tutorial, create a simple recipe that fits our categorical outcome of interest to all predictors (we’ll start with categorical and all predictors since that’s the closest to the shown example). For now, you can ignore the concept of roles and features they mention.

nausea_rec <- recipe(Nausea ~ ., data = train_data)
nausea_test <- recipe(Nausea ~ ., data = test_data)

#summary(ht_rec)
```

Set up logistic model and create workflow
```{r}
lr_model <- logistic_reg() %>%
  set_engine("glm")

nausea_workflow <- 
  workflow() %>%
  add_model(lr_model) %>%
  add_recipe(nausea_rec)
```

Show relationships between predictor variables and outcome 
```{r}
nausea_fit <- nausea_workflow %>%
  fit(data = train_data)

nausea_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```
The following code predicts whether a patient has nausea based on the model built above.
```{r results='hide'}
# use predict to predict if patient has nausea
predict(nausea_fit, test_data)
```

The following code shows the model's predictions for the test data set
```{r}
nausea_aug <- augment(nausea_fit, test_data)

nausea_aug %>% select(Nausea, .pred_class, .pred_Yes, .pred_No)
```

The results of the ROC curve the the ROC-AUC value of .72 shows the model is potentially useful for predicting nausea. 

```{r}
#Follow the example in the Use a trained workflow to predict section of the tutorial to look at the predictions, ROC and ROC-AUC for your data. Apply it to both the training and the test data. ROC and ROC-AUC is another common performance measure/metric for categorical outcomes. If you are not familiar with it, you can read more about them by following the link in the tutorial. It’s not too important to go into the details for now. The focus here is on getting the code to work. In general, a ROC-AUC of 0.5 means the model is no good, 1 is a perfect model. Generally, somewhere above 0.7 do people think the model might be useful.

nausea_aug %>%
  roc_curve(truth = Nausea, .pred_Yes, event_level= "second") %>%
  autoplot()
```

```{r}
nausea_aug %>% 
  roc_auc(truth = Nausea, .pred_Yes, event_level= "second")
```

## Testing the Main Predictor
### Runny Nose

The following model will test whether having a runny nose is a good predictor of nausea. Based on the results testing all variables as predictors of nausea, it is unlikely that having a runny nose will predict nausea (p-value of .699).
```{r}
#Let’s re-do the fitting but now with a model that only fits the main predictor to the categorical outcome. You should notice that the only thing you have to change is to set up a new recipe, this time one that only has the name of the predictor of interest on the right side of the formula (instead of the . symbol, which is shorthand notation for “all predictors”.) Then you can set up a new workflow with the new recipe, rerun the fit and evaluate performance using the same code as above. In general, if you do multiple models/recipes, you might want to write a loop to go over them, or parallelize/vectorize things. For now, just copying and pasting most of the code is ok.

# Create new recipes
RunnyNose_rec <- recipe(Nausea ~ RunnyNose, data = train_data)
RunnyNose_test <- recipe(Nausea ~ RunnyNose, data = test_data)
```

```{r}
RunnyNose_workflow <- 
  workflow() %>%
  add_model(lr_model) %>%
  add_recipe(RunnyNose_rec)
```

```{r}
RunnyNose_fit <- RunnyNose_workflow %>%
  fit(data = train_data)
```


```{r}
RunnyNose_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

```{r results = 'hide'}
# use predict to predict if patient has a high temperature
predict(RunnyNose_fit, test_data)
```

```{r}
RunnyNose_aug <- augment(RunnyNose_fit, test_data)

RunnyNose_aug %>% select(Nausea, .pred_class, .pred_Yes, .pred_No)
```

Once again, the ROC curve and ROC-AUC value (.52) indicate that RunnyNose might be a good variable for predicting Nausea, but is not strong enough of a model to be significant. 

```{r}
RunnyNose_aug %>%
  roc_curve(truth = Nausea, .pred_Yes, event_level= "second") %>%
  autoplot()
```

```{r}
RunnyNose_aug %>% 
  roc_auc(truth = Nausea, .pred_Yes, event_level= "second")
```


## Analysis Part 2

#### By Morgan Taylor

The goal of this section is to fit the continuous outcome of interest (BodyTemp) to a linear.  This is first tested with all present variables as predictors, then tested against the main predictor of interest, Runny Nose. Since the previous section already used the `Tidymodels` infrastructure, we can copy and paste a majority of the code, adjusting the outcome of interest and model type.

We can use the already loaded df, with the same test_data and train_data df.

## Testing All Predictors

The following code generates recipes for the training data set, then builds a workflow to fit to a linear model to all predictor variables. 

```{r}
# Next, following the example in the Create Recipes section of the Get Started tidymodels tutorial, create a simple recipe that fits our continuous outcome of interest to all predictors and ignore the concept of roles and features they mention.

bodytemp_rec <- recipe(BodyTemp ~ ., data = train_data)
```

Set up linear model and create workflow for training data
```{r}
linear_model <- linear_reg() %>%
                  set_engine("lm")

bodytemp_workflow <- 
  workflow() %>%
  add_model(linear_model) %>%
  add_recipe(bodytemp_rec)
```

Show relationships between predictor variables and outcome 
```{r}
bodytemp_fit <- bodytemp_workflow %>%
  fit(data = train_data)

bodytemp_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

The following code predicts a patient's body temperature based on the model built above for the training data set.
```{r}
# use predict to predict patient body temperature
predict(bodytemp_fit, train_data)

#create df with model predictions and actual measures
bodytemp_aug_train <- augment(bodytemp_fit, train_data)

bodytemp_all_train <- bodytemp_aug_train %>%
                        mutate(data = "train",
                               model = "all")
```

The following code predicts a patient's body temperature based on the model built above for the test data set.
```{r}
# use predict to predict patient body temperature
predict(bodytemp_fit, test_data)

#create df with model predictions and actual measures
bodytemp_aug_test <- augment(bodytemp_fit, test_data)

#fit into df for yardstick evaluation later
bodytemp_all_test <- bodytemp_aug_test %>%
                        mutate(data = "test",
                               model = "all")
```

## Testing Only Runny Nose

The following code generates recipes for the training data set, then builds a workflow to fit to a linear model to only the runny nose predictor variable. 

```{r}
# Next, following the example in the Create Recipes section of the Get Started tidymodels tutorial, create a simple recipe that fits our continuous outcome of interest to runny nose and ignore the concept of roles and features they mention.

bodytemp_rec_RN <- recipe(BodyTemp ~ RunnyNose, data = train_data)
```

Create workflow for RunnyNose (We can use the previously defined model)
```{r}
bodytemp_workflow_RN <- 
        workflow() %>%
        add_model(linear_model) %>%
        add_recipe(bodytemp_rec_RN)
```

Show relationships between predictor variable and outcome 
```{r}
bodytemp_fit_RN <- bodytemp_workflow_RN %>%
  fit(data = train_data)

bodytemp_fit_RN %>%
  extract_fit_parsnip() %>%
  tidy()
```

The following code predicts a patient's body temperature based on the model built above for the training data set.
```{r}
# use predict to predict patient body temperature
predict(bodytemp_fit_RN, train_data)

#create df with model predictions and actual measures
bodytemp_aug_train_RN <- augment(bodytemp_fit_RN, train_data)

bodytemp_RN_train <- bodytemp_aug_train_RN %>%
                        mutate(data = "train",
                               model = "RN")
```

The following code predicts a patient's body temperature based on the model built above for the test data set.
```{r}
# use predict to predict patient body temperature
predict(bodytemp_fit_RN, test_data)

#create df with model predictions and actual measures
bodytemp_aug_test_RN <- augment(bodytemp_fit_RN, test_data)

#fit into df for yardstick evaluation later
bodytemp_RN_test <- bodytemp_aug_test_RN %>%
                        mutate(data = "test",
                               model = "RN")
```


## Linear Model Evaluation

As this is now a linear regression model, we need to use RMSE instead of ROC as the metric. We can combine the predictions into one df and then calculate the RMSE for each model to compare. Inspiration for the coding for this section comes from the [TidyModels Yardstick Website](https://yardstick.tidymodels.org/)

```{r}
#First create comprehensive df with all of the predictions
bodytemp_preds <- bind_rows(bodytemp_all_test,
                            bodytemp_all_train,
                            bodytemp_RN_test,
                            bodytemp_RN_train)

#Use the yardstick package to calculate the metrics on resamples
bodytemp_metrics <- bodytemp_preds %>%
                      dplyr::group_by(data, model) %>%
                      yardstick::metrics(truth = BodyTemp, estimate = .pred)
#for interpretation: rmse = Root Mean Squared Error (RMSE), rsq = R^2, mae = Mean Absolute Error (MAE)

#make a df that just displays RMSE for each model and data (all dplyr functions)
bodytemp_RMSE <- bodytemp_metrics %>%
                    filter(.metric == "rmse") %>%
                    select(-.estimator)

#sort the df to make it more understandable
bodytemp_RMSE[order( bodytemp_RMSE[, 4, 2] ), ]
```

<br>

Interpreting the Results:
RMSE is the standard deviation of the unexplained variance, and has the same units as BodyTemp. A lower RMSE indicates a better fit. Based on the results above, we can conclude that the model that uses all of the predictors is a better fit than the model that only uses RunnyNose as the predictor. Interestingly, in the RN model, the test dataset had a lower RMSE than the training dataset, whereas in the all model, the training dataset had a lower RMSE than the test dataset.
