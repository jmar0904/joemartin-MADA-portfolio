---
title: "Tidy Tuesday 2"
output: 
  html_document:
    toc: FALSE
---
### Marble Racing
#### Tidy Tuesday - June 2, 2020

In the early days of the pandemic when professional sports leagues stopped play to slow the spread of COVID-19, Marble Racing became temporary replacement, earning Jon Oliver's endorsement on <em>Last Week Tonight</em>. While I'm sure there isn't big money to be earned gambling on marble races, <strong>I think it would be possible to predict race outcomes strictly based on qualifying performance. To play it safe, I would likely bet on whether a marble gets first, second, or third place, so the target variable will be top_3 (yes or no).</strong>

Before beginning this exercise, one limitation I with this data set are the number of features and the number of observations. Additionally, some features will have to be dropped to prevent data leakage. For example, I don't want to include the place each marble finished in a race as a predictor of whether it finished in the Top 3. 

In this Tidy Tuesday project, I'm going to wrangle the marble data set to include final place, explore the data to see if there is a trend, and build four models in an effort to find the best fit. 
```{r echo = FALSE}
# Load clean data from nberwp-master directory
pacman::p_load(pacman, tidyverse, here, lubridate)

read_file <- here::here("data","marbles","marbles.csv")
marbles <- read_csv(read_file)
```

```{r echo = FALSE, eval=FALSE}
glimpse(marbles)

#transform data into the right type
marbles$date <- dmy(marbles$date)
marbles$race <- as.factor(marbles$race)
marbles$site <- as.factor(marbles$site)
marbles$marble_name <- as.factor(trimws(marbles$marble_name))
marbles$team_name <- as.factor(marbles$team_name)
marbles$pole <- as.factor(marbles$pole)
marbles$host <- as.factor(marbles$host)

#drop source and notes variables
marbles <- marbles %>% select(-source,-notes)

summary(marbles)
```

Let's learn more about the marble league. Which are the highest-performing marbles? Teams? What are the tracks like?
```{r}
marble_points <- marbles %>%
  select(marble_name,team_name,points) %>%
  drop_na(points) %>%
  group_by(marble_name) %>%
  summarise(team_name,
            total = sum(points)) %>%
  arrange(desc(total)) %>%
  unique()
rmarkdown::paged_table(marble_points) 
```

```{r}
team_points <- marbles %>%
  select(team_name,points) %>%
  drop_na(points) %>%
  group_by(team_name) %>%
  summarise(total = sum(points)) %>%
  arrange(desc(total))
rmarkdown::paged_table(team_points)
```

How fast is each track? 
```{r}
tracks <- marbles %>%
  select(site,track_length_m,avg_time_lap) %>%
  drop_na() %>%
  group_by(site) %>%
  summarize(Length = track_length_m,
            `Average Speed (seconds)` = mean(avg_time_lap)) %>%
  arrange(desc(`Average Speed (seconds)`)) %>%
  unique()
rmarkdown::paged_table(tracks)  
```

```{r}
race_results <- # dataframe to see race results only
  marbles %>% drop_na(points)

#create a variable for finishing place
place <- rep(1:16,8)
race_results$place <- place

qualifiers <- marbles %>% filter(grepl("Q",marbles$race)) #dataframe to see qualifier results only
qualifiers$place <- qualifiers$pole #add pole place to the place variable
qualifiers$place <- as.numeric(str_remove(qualifiers$place,"P"))

marbles2 <- bind_rows(race_results, qualifiers) # put tables together to have one variable for the final place
marbles2$type <- ifelse(grepl("Q",marbles2$race),"qualifier","race") # create a variable that indicates if it's a qualifier or a race
marbles2$race <- as.character(marbles2$race)
marbles2$event <- substr(marbles2$race,4,4)
```

```{r}
marbles_race <- marbles2 %>% filter(type == "race") %>%
  select(-date,-race,-pole,-points, -type)#trying to get one variable with the qualifying time, qualifying place, race time, race place
marbles_qual <- marbles2 %>% filter(type == "qualifier")%>%
  select(-date,-race,-pole,-points, -type)

marbles_race <- marbles_race %>% rename("race_place" = place)
marbles_race <- marbles_race %>% rename("race_time" = time_s)
marbles_race <- marbles_race %>% rename("race_laps" = number_laps)

marbles_qual <- marbles_qual %>% rename("qual_place" = place)
marbles_qual <- marbles_qual %>% rename("qual_time" = time_s)
marbles_qual <- marbles_qual %>% rename("qual_laps" = number_laps)
marbles_qual <- marbles_qual %>% select(event,marble_name, qual_time, qual_place, qual_laps)

# join on event and marble name
marbles3 <- full_join(marbles_race,marbles_qual, by = c("event", "marble_name")) # this dataset will enable me to use qualifying place as a variable for each corresponding race

# race_place should be a factor to fit into a logistic model
marbles3$race_place <- as.factor(marbles3$race_place)
marbles3$top_3 <- as.factor(ifelse(marbles3$race_place == 1 |marbles3$race_place == 2 | marbles3$race_place == 3,
                         1,0))
gg_marb <- marbles3
gg_marb$top_3 <- as.numeric(gg_marb$top_3)

m4 <- marbles3 %>% select(-race_place,-race_time,-avg_time_lap, -event, -qual_laps) #get rid of variables that would cause data leak and variables that aren't relavent to prediction. 
```

Looking at the top finishers for each event, it's unlikely for the winner of each qualifying event to win the corresponding race. However, it is possible that 
```{r}
marbles2 %>%
  filter(place == 1)%>%
  ggplot(aes(x=event,y=type, label=marble_name))+
  geom_point()+
  geom_label(aes(fill = team_name))+
  labs(title = "Are top qualifiers most likely to win?", 
       subtitle = "Top finishers for each qualifier and race",
       x = "Race",
       y = "")+
  theme_classic()+
  theme(legend.position = "none")
```

How did individual teammate performances compare? Looking at this graphic helps to show roughly which individual marbles finish consistently high and which finish consistently low.
```{r}
# inspiration from @joepope44 https://twitter.com/joepope44/status/1268727311450472448
marble_points %>%
  ggplot(aes(x=total, y=team_name, label = marble_name))+ # try sorting y axis based on season performance
  geom_point() +
  geom_line() +
  geom_label(aes(fill = team_name))+
  labs(title = "Team Performance", 
       subtitle = "Individual team contribution to team points",
       x = "Points", y = "Team")+
  theme_classic()+
  theme(legend.position = "none")
```

Who has the most top 3 finishes? Looks like Snowy, Speedy, Smoggy, Rapidly, Orangin, and Hazy have the greatest number of top 3 finishes. 
```{r}
gg_marb %>% ggplot(aes(x = marble_name, y = top_3))+
  geom_col()+
  coord_flip()+
  theme_classic()+
  labs(title = "Top 3 Finishes", 
       subtitle = "How many top 3 finishes has each marble achieved",
       x = "Top 3 Finishes", y = "Marble")
```

```{r}
# Start building a model
p_load(tidymodels)

set.seed(123)

data_split <- initial_split(m4, prop = 3/4)
train_data <- training(data_split)
test_data <- testing(data_split)

m_rec <- recipe(top_3 ~ ., data = train_data)%>%
  step_dummy(all_nominal_predictors())
```

```{r}
lr_mod <- 
  logistic_reg() %>%
  set_engine("glm")

m_wf <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(m_rec)

m_fit <- 
  m_wf %>%
  fit(data = train_data)

tidy(m_fit) #will throw a warning
```

```{r warning = FALSE}
predict(m_fit,test_data) #will throw a warning

m_aug <- augment(m_fit,test_data)

m_aug %>%
  select(qual_place,top_3,.pred_class)

m_aug$.pred_class <- as.numeric(m_aug$.pred_class,event_level= "second")
```

```{r warning = FALSE}
m_aug %>%
  roc_curve(truth = top_3, .pred_class, event_level= "second") %>%
  autoplot()

m_aug %>% roc_auc(truth = top_3, .pred_class, event_level = "second")
```

This model does not quite predict results very well, however, it did to better than I hoped. Next, I'll try to see how it does with retraining. A LASSO or ridge regression model is probably going to be my final choice because there are fewer features in this data set. I'll start with a decision tree and then I'll try a random forest. I'll continue to use the same data split and recipe. 

```{r warning = FALSE, error = FALSE, message= FALSE}
folds <- vfold_cv(train_data, v = 5, repeats = 5, strata = top_3)

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)

mtree_wf <- workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(m_rec)

m_tree_res <- mtree_wf %>%
  tune_grid(resamples = folds,
            grid = tree_grid)

m_tree_res %>%
  collect_metrics()

m_tree_res %>%
  show_best("roc_auc", event_level = "second")

# select best roc_auc
best_model <- m_tree_res %>%
  select_best("roc_auc")

final_wf <- mtree_wf %>%
  finalize_workflow(best_model)

final_fit <- final_wf %>%
  last_fit(data_split)

final_fit[[5]][[1]]$.pred_class <- as.numeric(final_fit[[5]][[1]]$.pred_class)

final_fit %>%
  collect_predictions() %>% 
  roc_curve(top_3, .pred_class, event_level="second") %>% 
  autoplot()

final_fit %>%
  collect_predictions() %>%
  roc_auc(top_3, .pred_class, event_level = "second")
```

The tree model has the same roc_auc value as the previous model. Next, I tried to build a random forest, but my code did not cooperate and threw an error. After this I opted for LASSO and Ridge Regression models. 
```{r warning = FALSE}
# this section uses the same data split and vfold cross-validation as the previous models
#val_set <- validation_split(train_data,
                            #strata = top_3,
                            #prop = 8/10)

#cores <- parallel::detectCores()

#rf_marb <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
 # set_engine("ranger", num.threads = cores) %>%
 # set_mode("classification")

#rf_marb_wf <- workflow() %>%
 # add_model(rf_marb) %>%
 # add_recipe(m_rec)
```

```{r r17, warning = FALSE, error = FALSE, message= FALSE}
#tune grid
#rf_res <- rf_marb_wf %>%
 # tune_grid(val_set,
 # grid = 25,
 # control = control_grid(save_pred = TRUE),
 # metrics = metric_set(roc_auc))
```

```{r warning = FALSE}
#rf_best <- rf_res %>%
 # select_best(metric = "roc_auc")
#rf_best

#rf_auc <- 
 #rf_res %>% 
 #collect_predictions(parameters = rf_best) %>% 
  #roc_curve(top_3, .pred_1) %>% 
  #mutate(model = "Random Forest")

#rf_auc
```


```{r warning = FALSE}
#Looking at the results from the random forest, it seems that this was not a good model. In fact, the performance of the previous models seems to be better. 

#rf_auc %>%
#ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) +
  #geom_path() +
  #geom_abline() + 
  #coord_equal()
```

Create LASSO model
```{r , warning = FALSE, error = FALSE, message= FALSE}
# LASSO
# continue using same data split and model engine

lasso_wf <- 
  workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(m_rec)

lasso_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lasso_res <- # this will throw a warning 
  lasso_wf %>%
  tune_grid(folds,
            grid = lasso_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

Evaluate Lasso model. The best LASSO model results in a roc_auc value of .51. This is a better result than .46 roc auc value from the decision tree, but still not quite accurate enough. 
```{r}
lasso_res %>%
  show_best("roc_auc")

# select best
lasso_best <- lasso_res %>%
  select_best("roc_auc")
lasso_best

# finalize workflow
final_lasso_wf <- 
  lasso_wf %>% 
  finalize_workflow(lasso_best)

# final fit with split data
final_lasso_fit <- final_lasso_wf %>%
  last_fit(data_split)

final_lasso_fit %>%
  collect_metrics()

```

At the beginning of this page, I addressed the fact that there is very little data here. Considering I'm trying to use qualification data to predict race outcomes, there are only 128 observations. Thinking about this problem, I though that a ridge regression model might improve performance a bit. 

```{r warning = FALSE, error = FALSE, message= FALSE}
# using the same data split from earlier
r_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

ridge_reg <- logistic_reg(
  mode = "classification",
  engine = "glm",
  mixture = 0) #setting mixture to 0 indicates a ridge regression is being used

r_wf <- 
  workflow() %>%
  add_model(ridge_reg) %>%
  add_recipe(m_rec)

r_res <- # this will throw a warning saying there are no tuning parameters
  r_wf %>%
  tune_grid(folds,
            grid = r_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

```{r warning = FALSE}
r_res %>%
  show_best("roc_auc", event_level = "second")

# select best
r_best <- r_res %>%
  select_best("roc_auc", event_level = "second")
r_best

# finalize workflow
final_r_wf <- 
  r_wf %>% 
  finalize_workflow(r_best)

# final fit with split data
final_r_fit <- final_r_wf %>%
  last_fit(data_split)

final_r_fit %>%
  collect_metrics()

```

In conclusion, LASSO and Ridge regressions were the best of all the models used. However, with a ROC-AUC value of just .45 for each and an accuracy of 62%, they still isn't strong enough of a model to be useful in predicting the top three finishers in the marble races. At least, this isn't something I would rely on if I were putting money on these races. 

More than anything else, this model would be strengthened by a greater number of observations and perhaps a greater number of variables. With just 128 observations and 8 predictor variables, it is difficult to get accurate results. 