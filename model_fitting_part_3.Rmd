---
title: "Analysis of University Health Center Patients"
output: 
  html_document:
    toc: FALSE
---
<em>This is the final analysis in a series of three. To view previous analyses leading to this one, please visit <a href="https://github.com/jmar0904/joemartin-MADA-analysis3/tree/master/code/analysis_code">my GitHub repository</a>.</em>

```{r echo = FALSE}
# Create a Linear Regression with the target variable BodyTemp
pacman::p_load(pacman,tidymodels,tidyverse,here, rpart, glmnet, ranger, rpart.plot)

#load Mod 11 data
df <- read_rds(here::here("files", "processeddata_mod11.rds"))
```


Create model linear regression model. Train and test sets will have a 70/30 split. In this section, create the cross-fold validation object. 5 folds repeated 5 times. Data sets for split and folds will be stratified on BodyTemp variable.
```{r}
set.seed(123)

# Set up for Linear Regression
df_split <- initial_split(df, strata = BodyTemp, prop=7/10)

training_df <- training(df_split)
testing_df <- testing(df_split)

#Set up for cross-fold validation
folds <- vfold_cv(training_df, v = 5, repeats = 5, strata = BodyTemp)

# create recipe
bt_train_rec <- recipe(BodyTemp ~ ., data = training_df) %>%
  step_dummy(all_nominal_predictors())

# Use step_dummy to transform categorical variables into dummy variables

lm_mod <- linear_reg() %>%
  set_engine("lm")

bt_workflow <- 
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(bt_train_rec)

bt_fit <- bt_workflow %>%
  fit(training_df)

bt_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

Evaluate model performance
```{r}
# Check the model performance

predict(bt_fit, testing_df)

bt_aug_train <- augment(bt_fit, training_df)
bt_aug_train %>% select(BodyTemp, .pred)
bt_aug_test <- augment(bt_fit,testing_df)
bt_aug_test %>% select(BodyTemp, .pred)

bt_error_train <- bt_aug_train %>%
  rmse(truth = BodyTemp, .pred)
bt_error_train$model <- "BodyTemp Training"
bt_error_test <- bt_aug_test %>%
  rmse(truth = BodyTemp, .pred)
bt_error_test$model <- "BodyTemp Test"
bt_error_rmse <- bind_rows(bt_error_train, bt_error_test)
bt_error_rmse
```

Create a null model to compare the linear regression model. 
```{r}
# Create a null model
null_rec <- recipe(BodyTemp ~ 1, data = training_df) %>% 
  step_dummy(all_nominal_predictors())

null_wf <- 
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(null_rec)

null_fit <- null_wf %>%
  fit(training_df)

null_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

Evaluate model performance.
```{r}
# Check Null Model performance
predict(null_fit, testing_df)

null_aug_train <- augment(null_fit, training_df)
null_aug_train %>% select(BodyTemp, .pred)
null_aug_test <- augment(null_fit,testing_df)
null_aug_test %>% select(BodyTemp, .pred)

null_error_train <- null_aug_train %>%
  rmse(truth = BodyTemp, .pred)
null_error_train$model <- "Null Training Model"


null_error_test <- null_aug_test %>%
  rmse(truth = BodyTemp, .pred)
null_error_test$model <- "Null Testing Model"

null_error_rmse <- bind_rows(null_error_train,null_error_test)
null_error_rmse
```

Create a decision tree. Use the same data sets for training and the same cross-fold validation object.
```{r warning = FALSE, error = FALSE, message= FALSE}
# decision tree
# continue using training_df and testing_df for data sets

tune_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune()
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)

# create tree workflow with bodytemp train recipe
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(bt_train_rec)

# create tree with new workflow
tree_res <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid
  )

tree_res %>%
  collect_metrics()
```

Evaluate for the best model. The best-performing model has RMSE of 1.187.
```{r}
tree_res %>%
  show_best("rmse")

# autoplot tree_res. This will show what happened during the tuning process.
tree_res %>% autoplot()

best_tree <- tree_res %>%
  select_best("rmse")

#finalize workflow
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf

# final fit - MADA instructions say with training data, but function requires split data
final_fit <- 
  final_wf %>%
  last_fit(df_split)

final_fit %>%
  collect_metrics()

final_fit %>%
  collect_predictions() %>%
  rmse(BodyTemp, .pred)

final_tree <- extract_workflow(final_fit)
final_tree

# plot tree with rpart.plot
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

Create LASSO model
```{r}
# LASSO
# continue using same data split and linear regression model
# continue using same bt_train_rec

lasso_wf <- 
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(bt_train_rec)

lasso_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lasso_res <- # this will throw a warning saying there are no tuning parameters
  lasso_wf %>%
  tune_grid(folds,
            grid = lasso_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

Evaluate Lasso model. The highest-performing model has RMSE of 1.199
```{r}
lasso_res %>%
  show_best("rmse")

# select best
lasso_best <- lasso_res %>%
  select_best("rmse")
lasso_best

# finalize workflow
final_lasso_wf <- 
  lasso_wf %>% 
  finalize_workflow(lasso_best)

# final fit with split data
final_lasso_fit <- final_lasso_wf %>%
  last_fit(df_split)

final_lasso_fit %>%
  collect_metrics()

```
Create random forest using the same data sets and the same cross-validation object. 
```{r}
# Forest
# use same dataset and recipes
# how many cores does my computer have?
cores <- parallel::detectCores()
cores
```

Set the model engine. Leaving mtry and min_n eqult to tune() for now. Running the model this way will help evaluate how to tune the model using these parameters.
```{r}
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(bt_train_rec)

rf_mod %>% parameters()

rf_res <- rf_wf %>%
  tune_grid(folds,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```

Evaluate the random forest. The best-performing model has RMSE of 1.177.
```{r}
rf_res %>%
  show_best(metric = "rmse")

rf_res %>% 
  collect_predictions()

# autoplot to show how the model was tuned
autoplot(rf_res)

#select best
rf_best <- 
  rf_res %>%
  select_best(metric = "rmse")
rf_best

#finalize workflow
final_rf_wf <- 
  rf_wf %>% 
  finalize_workflow(rf_best)

# final fit with split data
final_fit_rf <- final_rf_wf %>%
  last_fit(df_split)

final_fit_rf %>%
  collect_metrics()
```

After completing the tree, LASSO, and random forest models, LASSO had the greatest RMSE (~1.2) and the random forest had the lowest RMSE (~1.77). Based on this alone the best model seems to be the random forest. It's also important to note that the random forest seems to be a much more robust approach than the other two. Given the high number of repetitions (1,000 trees), it would make sense that this model performs better than the others. The final part of this project will utilize test data to evaluate how the random forest model performs.

```{r}
rf_last_mod <- 
  rand_forest(mtry = 6,min_n = 28, trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")

rf_last_wf <- 
  final_rf_wf %>%
  update_model(rf_last_mod)

# using split data again (can't specifically use test data)
rf_last_fit <- 
  rf_last_wf %>%
  last_fit(df_split)

collect_metrics(rf_last_fit)
```

The RMSE of the random forest model is 1.18, which is still among the best scores of all the models tried. This final test does show that the random forest seems to be the most robust model created, compared to the LASSO and the decision tree.